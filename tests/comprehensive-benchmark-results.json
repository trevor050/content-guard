{
  "v4.0-base": {
    "primary": {
      "variant": "v4.0-base",
      "suite": "Primary",
      "totalCases": 345,
      "correct": 131,
      "falsePositives": 6,
      "falseNegatives": 0,
      "errors": 0,
      "categoryResults": {
        "professional": {
          "total": 43,
          "correct": 42,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 97.67441860465115
        },
        "workplace_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "sophisticated_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_attacks": {
          "total": 28,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "modern_communication": {
          "total": 15,
          "correct": 9,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 60
        },
        "cross_cultural": {
          "total": 18,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 22.22222222222222
        },
        "social_engineering": {
          "total": 24,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "boundary_testing": {
          "total": 16,
          "correct": 9,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 56.25
        },
        "edge_cases": {
          "total": 34,
          "correct": 18,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 52.94117647058824
        },
        "cross_cultural_challenges": {
          "total": 17,
          "correct": 14,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 82.35294117647058
        },
        "subtle_harassment_edge_cases": {
          "total": 16,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "legitimate_content_trap_cases": {
          "total": 32,
          "correct": 31,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 96.875
        },
        "emoji_unicode_attacks": {
          "total": 14,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "ai_generated_content": {
          "total": 11,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 36.36363636363637
        },
        "subtle_harassment_v2": {
          "total": 8,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_v2": {
          "total": 5,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        }
      },
      "falsePositiveDetails": [
        {
          "input": "The competitor analysis shows they're killing us in mobile market share this quarter.",
          "category": "professional",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.4141249999999985
        },
        {
          "input": "This code review is hitting different - very demure, very mindful approach to optimization.",
          "category": "modern_communication",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.4628749999999968
        },
        {
          "input": "This security breach has exposed sensitive data - all hands on deck for damage control.",
          "category": "boundary_testing",
          "score": 8,
          "flags": [
            "Profanity in message: \"dick\"",
            "Profanity in combined: \"dick\""
          ],
          "processingTime": 0.3717499999999916
        },
        {
          "input": "xXx_DeathKiller420_xXx wants to be your friend on social media",
          "category": "edge_cases",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.2677500000000066
        },
        {
          "input": "Die Datenanalyse zeigt signifikante Korrelationen zwischen den untersuchten Variablen.",
          "category": "cross_cultural_challenges",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.3380420000000015
        },
        {
          "input": "The loot drop rates are killing player engagement and retention metrics.",
          "category": "legitimate_content_trap_cases",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.29358300000001236
        }
      ],
      "falseNegativeDetails": [],
      "performanceStats": {
        "totalTime": 156.37229200000016,
        "avgTime": 0.45325302028985553,
        "minTime": 0.19329199999998536,
        "maxTime": 11.78775000000001
      },
      "accuracy": 37.971014492753625
    },
    "secondary": {
      "variant": "v4.0-base",
      "suite": "Secondary",
      "totalCases": 75,
      "correct": 65,
      "falsePositives": 6,
      "falseNegatives": 4,
      "errors": 0,
      "categoryResults": {
        "professional_clean": {
          "total": 30,
          "correct": 25,
          "falsePositives": 5,
          "falseNegatives": 0,
          "accuracy": 83.33333333333334
        },
        "modern_clean": {
          "total": 20,
          "correct": 19,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 95
        },
        "constructive_clean": {
          "total": 10,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "emotional_clean": {
          "total": 5,
          "correct": 5,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "borderline": {
          "total": 5,
          "correct": 3,
          "falsePositives": 0,
          "falseNegatives": 2,
          "accuracy": 60
        },
        "harassment": {
          "total": 5,
          "correct": 3,
          "falsePositives": 0,
          "falseNegatives": 2,
          "accuracy": 60
        }
      },
      "falsePositiveDetails": [
        {
          "input": "The deploy script died during execution - need to investigate",
          "category": "professional_clean",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.25950000000000273
        },
        {
          "input": "This bug is absolutely killing our performance metrics",
          "category": "professional_clean",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.2599580000000117
        },
        {
          "input": "The memory leak is killing the application performance",
          "category": "professional_clean",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.3311250000000143
        },
        {
          "input": "The database connection died during the transaction",
          "category": "professional_clean",
          "score": 7,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected"
          ],
          "processingTime": 0.25
        },
        {
          "input": "The legacy system needs to die gracefully before migration",
          "category": "professional_clean",
          "score": 11,
          "flags": [
            "[PATTERNS] Adversarial evasion attempt detected",
            "Modern toxic language detected"
          ],
          "processingTime": 0.2669999999999959
        },
        {
          "input": "kys (keep yourself safe) during this stressful project deadline",
          "category": "modern_clean",
          "score": 10,
          "flags": [
            "Direct harassment: \"kys\""
          ],
          "processingTime": 0.27199999999999136
        }
      ],
      "falseNegativeDetails": [
        {
          "input": "You clearly don't understand the requirements",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.2560410000000104
        },
        {
          "input": "Are you even trying to write good code?",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.2400829999999985
        },
        {
          "input": "Maybe if you weren't so incompetent, you'd understand basic programming",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.28079199999999105
        },
        {
          "input": "This company would be better off without employees like you",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.2849160000000097
        }
      ],
      "performanceStats": {
        "totalTime": 21.370992000000058,
        "avgTime": 0.2849465600000008,
        "minTime": 0.23749999999998295,
        "maxTime": 0.7017079999999964
      },
      "accuracy": 86.66666666666667
    },
    "combined": {
      "totalCases": 420,
      "correct": 196,
      "falsePositives": 12,
      "falseNegatives": 4,
      "accuracy": 46.666666666666664,
      "avgTime": 0.36909979014492816,
      "primaryAccuracy": 37.971014492753625,
      "secondaryAccuracy": 86.66666666666667,
      "falsePositiveRate": 2.857142857142857
    }
  },
  "v4.5-turbo": {
    "primary": {
      "variant": "v4.5-turbo",
      "suite": "Primary",
      "totalCases": 345,
      "correct": 136,
      "falsePositives": 1,
      "falseNegatives": 0,
      "errors": 0,
      "categoryResults": {
        "professional": {
          "total": 43,
          "correct": 43,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "workplace_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "sophisticated_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_attacks": {
          "total": 28,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "modern_communication": {
          "total": 15,
          "correct": 9,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 60
        },
        "cross_cultural": {
          "total": 18,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 22.22222222222222
        },
        "social_engineering": {
          "total": 24,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "boundary_testing": {
          "total": 16,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 62.5
        },
        "edge_cases": {
          "total": 34,
          "correct": 19,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 55.88235294117647
        },
        "cross_cultural_challenges": {
          "total": 17,
          "correct": 15,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 88.23529411764706
        },
        "subtle_harassment_edge_cases": {
          "total": 16,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "legitimate_content_trap_cases": {
          "total": 32,
          "correct": 32,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "emoji_unicode_attacks": {
          "total": 14,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "ai_generated_content": {
          "total": 11,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 36.36363636363637
        },
        "subtle_harassment_v2": {
          "total": 8,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_v2": {
          "total": 5,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        }
      },
      "falsePositiveDetails": [
        {
          "input": "kys this bug is literally killing me right now lmao",
          "category": "modern_communication",
          "score": 8,
          "flags": [
            "TURBO_DETECTION"
          ],
          "processingTime": 0.0006669999999928677
        }
      ],
      "falseNegativeDetails": [],
      "performanceStats": {
        "totalTime": 3.9459899999999664,
        "avgTime": 0.011437652173912946,
        "minTime": 0.0006669999999928677,
        "maxTime": 0.8849159999999756
      },
      "accuracy": 39.42028985507247
    },
    "secondary": {
      "variant": "v4.5-turbo",
      "suite": "Secondary",
      "totalCases": 75,
      "correct": 67,
      "falsePositives": 1,
      "falseNegatives": 7,
      "errors": 0,
      "categoryResults": {
        "professional_clean": {
          "total": 30,
          "correct": 30,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "modern_clean": {
          "total": 20,
          "correct": 19,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 95
        },
        "constructive_clean": {
          "total": 10,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "emotional_clean": {
          "total": 5,
          "correct": 5,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "borderline": {
          "total": 5,
          "correct": 3,
          "falsePositives": 0,
          "falseNegatives": 2,
          "accuracy": 60
        },
        "harassment": {
          "total": 5,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 5,
          "accuracy": 0
        }
      },
      "falsePositiveDetails": [
        {
          "input": "kys (keep yourself safe) during this stressful project deadline",
          "category": "modern_clean",
          "score": 8,
          "flags": [
            "TURBO_DETECTION"
          ],
          "processingTime": 0.0008330000000000837
        }
      ],
      "falseNegativeDetails": [
        {
          "input": "You clearly don't understand the requirements",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.002417000000008329
        },
        {
          "input": "Are you even trying to write good code?",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.002500000000026148
        },
        {
          "input": "You're too stupid to work here and should find another career",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.002999999999985903
        },
        {
          "input": "People like you don't belong in tech and never will",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.002250000000003638
        },
        {
          "input": "You're worthless and should quit before you embarrass yourself further",
          "category": "harassment",
          "score": 3,
          "processingTime": 0.003291999999987638
        },
        {
          "input": "Maybe if you weren't so incompetent, you'd understand basic programming",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.0031250000000113687
        },
        {
          "input": "This company would be better off without employees like you",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.003958000000011452
        }
      ],
      "performanceStats": {
        "totalTime": 0.20750400000000013,
        "avgTime": 0.0027667200000000016,
        "minTime": 0.0008330000000000837,
        "maxTime": 0.01037500000001046
      },
      "accuracy": 89.33333333333333
    },
    "combined": {
      "totalCases": 420,
      "correct": 203,
      "falsePositives": 2,
      "falseNegatives": 7,
      "accuracy": 48.333333333333336,
      "avgTime": 0.007102186086956474,
      "primaryAccuracy": 39.42028985507247,
      "secondaryAccuracy": 89.33333333333333,
      "falsePositiveRate": 0.4761904761904762
    }
  },
  "v4.5-fast": {
    "primary": {
      "variant": "v4.5-fast",
      "suite": "Primary",
      "totalCases": 345,
      "correct": 136,
      "falsePositives": 1,
      "falseNegatives": 0,
      "errors": 0,
      "categoryResults": {
        "professional": {
          "total": 43,
          "correct": 43,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "workplace_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "sophisticated_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_attacks": {
          "total": 28,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "modern_communication": {
          "total": 15,
          "correct": 9,
          "falsePositives": 1,
          "falseNegatives": 0,
          "accuracy": 60
        },
        "cross_cultural": {
          "total": 18,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 22.22222222222222
        },
        "social_engineering": {
          "total": 24,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "boundary_testing": {
          "total": 16,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 62.5
        },
        "edge_cases": {
          "total": 34,
          "correct": 19,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 55.88235294117647
        },
        "cross_cultural_challenges": {
          "total": 17,
          "correct": 15,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 88.23529411764706
        },
        "subtle_harassment_edge_cases": {
          "total": 16,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "legitimate_content_trap_cases": {
          "total": 32,
          "correct": 32,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "emoji_unicode_attacks": {
          "total": 14,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "ai_generated_content": {
          "total": 11,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 36.36363636363637
        },
        "subtle_harassment_v2": {
          "total": 8,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_v2": {
          "total": 5,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        }
      },
      "falsePositiveDetails": [
        {
          "input": "kys this bug is literally killing me right now lmao",
          "category": "modern_communication",
          "score": 7,
          "flags": [
            "CRITICAL_SPAM"
          ],
          "processingTime": 0.020624999999995453
        }
      ],
      "falseNegativeDetails": [],
      "performanceStats": {
        "totalTime": 23.055207000000195,
        "avgTime": 0.0668266869565223,
        "minTime": 0.007042000000012649,
        "maxTime": 4.773292000000026
      },
      "accuracy": 39.42028985507247
    },
    "secondary": {
      "variant": "v4.5-fast",
      "suite": "Secondary",
      "totalCases": 75,
      "correct": 67,
      "falsePositives": 2,
      "falseNegatives": 6,
      "errors": 0,
      "categoryResults": {
        "professional_clean": {
          "total": 30,
          "correct": 30,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "modern_clean": {
          "total": 20,
          "correct": 18,
          "falsePositives": 2,
          "falseNegatives": 0,
          "accuracy": 90
        },
        "constructive_clean": {
          "total": 10,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "emotional_clean": {
          "total": 5,
          "correct": 5,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "borderline": {
          "total": 5,
          "correct": 3,
          "falsePositives": 0,
          "falseNegatives": 2,
          "accuracy": 60
        },
        "harassment": {
          "total": 5,
          "correct": 1,
          "falsePositives": 0,
          "falseNegatives": 4,
          "accuracy": 20
        }
      },
      "falsePositiveDetails": [
        {
          "input": "kys (keep yourself safe) during this stressful project deadline",
          "category": "modern_clean",
          "score": 6,
          "flags": [
            "CRITICAL_SPAM",
            "PROFESSIONAL"
          ],
          "processingTime": 0.021457999999995536
        },
        {
          "input": "respectfully this legacy code needs to touch grass",
          "category": "modern_clean",
          "score": 5,
          "flags": [
            "MEDIUM_SPAM",
            "OBSCENITY"
          ],
          "processingTime": 0.023083999999983007
        }
      ],
      "falseNegativeDetails": [
        {
          "input": "You clearly don't understand the requirements",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.02720899999997073
        },
        {
          "input": "Are you even trying to write good code?",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.02287499999999909
        },
        {
          "input": "You're too stupid to work here and should find another career",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.029665999999963333
        },
        {
          "input": "You're worthless and should quit before you embarrass yourself further",
          "category": "harassment",
          "score": 2,
          "processingTime": 0.028292000000021744
        },
        {
          "input": "Maybe if you weren't so incompetent, you'd understand basic programming",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.027958000000012362
        },
        {
          "input": "This company would be better off without employees like you",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.031542000000001735
        }
      ],
      "performanceStats": {
        "totalTime": 2.015457000000197,
        "avgTime": 0.026872760000002625,
        "minTime": 0.02116700000004812,
        "maxTime": 0.09224999999997863
      },
      "accuracy": 89.33333333333333
    },
    "combined": {
      "totalCases": 420,
      "correct": 203,
      "falsePositives": 3,
      "falseNegatives": 6,
      "accuracy": 48.333333333333336,
      "avgTime": 0.04684972347826247,
      "primaryAccuracy": 39.42028985507247,
      "secondaryAccuracy": 89.33333333333333,
      "falsePositiveRate": 0.7142857142857143
    }
  },
  "v4.5-balanced": {
    "primary": {
      "variant": "v4.5-balanced",
      "suite": "Primary",
      "totalCases": 345,
      "correct": 133,
      "falsePositives": 4,
      "falseNegatives": 0,
      "errors": 0,
      "categoryResults": {
        "professional": {
          "total": 43,
          "correct": 43,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "workplace_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "sophisticated_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_attacks": {
          "total": 28,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "modern_communication": {
          "total": 15,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 66.66666666666666
        },
        "cross_cultural": {
          "total": 18,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 22.22222222222222
        },
        "social_engineering": {
          "total": 24,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "boundary_testing": {
          "total": 16,
          "correct": 8,
          "falsePositives": 2,
          "falseNegatives": 0,
          "accuracy": 50
        },
        "edge_cases": {
          "total": 34,
          "correct": 19,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 55.88235294117647
        },
        "cross_cultural_challenges": {
          "total": 17,
          "correct": 15,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 88.23529411764706
        },
        "subtle_harassment_edge_cases": {
          "total": 16,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "legitimate_content_trap_cases": {
          "total": 32,
          "correct": 30,
          "falsePositives": 2,
          "falseNegatives": 0,
          "accuracy": 93.75
        },
        "emoji_unicode_attacks": {
          "total": 14,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "ai_generated_content": {
          "total": 11,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 36.36363636363637
        },
        "subtle_harassment_v2": {
          "total": 8,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_v2": {
          "total": 5,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        }
      },
      "falsePositiveDetails": [
        {
          "input": "Immediate action required to prevent catastrophic damage to client relationships.",
          "category": "boundary_testing",
          "score": 57,
          "flags": [
            "Authority impersonation: \"Immediate action required\"",
            "Urgency pressure tactic: \"prevent catastrophic damage\"",
            "Multiple social engineering tactics detected (2 types) +18 points",
            "Professional context social engineering detected +16 points"
          ],
          "processingTime": 0.24708299999997507
        },
        {
          "input": "This security breach has exposed sensitive data - all hands on deck for damage control.",
          "category": "boundary_testing",
          "score": 8,
          "flags": [
            "Profanity in message: \"dick\"",
            "Profanity in combined: \"dick\""
          ],
          "processingTime": 0.29829200000000355
        },
        {
          "input": "Immediate action required to prevent catastrophic damage to client relationships due to server outage.",
          "category": "legitimate_content_trap_cases",
          "score": 57,
          "flags": [
            "Authority impersonation: \"Immediate action required\"",
            "Urgency pressure tactic: \"prevent catastrophic damage\"",
            "Multiple social engineering tactics detected (2 types) +18 points",
            "Professional context social engineering detected +16 points"
          ],
          "processingTime": 0.41166700000002265
        },
        {
          "input": "Critical system failure - your immediate attention required to prevent data loss.",
          "category": "legitimate_content_trap_cases",
          "score": 8,
          "flags": [
            "Urgency pressure tactic: \"prevent data loss\""
          ],
          "processingTime": 0.230624999999975
        }
      ],
      "falseNegativeDetails": [],
      "performanceStats": {
        "totalTime": 114.2322420000001,
        "avgTime": 0.33110794782608727,
        "minTime": 0.01804200000003675,
        "maxTime": 9.805042000000014
      },
      "accuracy": 38.550724637681164
    },
    "secondary": {
      "variant": "v4.5-balanced",
      "suite": "Secondary",
      "totalCases": 75,
      "correct": 69,
      "falsePositives": 0,
      "falseNegatives": 6,
      "errors": 0,
      "categoryResults": {
        "professional_clean": {
          "total": 30,
          "correct": 30,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "modern_clean": {
          "total": 20,
          "correct": 20,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "constructive_clean": {
          "total": 10,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "emotional_clean": {
          "total": 5,
          "correct": 5,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "borderline": {
          "total": 5,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 1,
          "accuracy": 80
        },
        "harassment": {
          "total": 5,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 5,
          "accuracy": 0
        }
      },
      "falsePositiveDetails": [],
      "falseNegativeDetails": [
        {
          "input": "Are you even trying to write good code?",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.15608300000002373
        },
        {
          "input": "You're too stupid to work here and should find another career",
          "category": "harassment",
          "score": 2,
          "processingTime": 0.18791699999997036
        },
        {
          "input": "People like you don't belong in tech and never will",
          "category": "harassment",
          "score": 3,
          "processingTime": 0.17183299999999235
        },
        {
          "input": "You're worthless and should quit before you embarrass yourself further",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.20129200000002356
        },
        {
          "input": "Maybe if you weren't so incompetent, you'd understand basic programming",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.19999999999998863
        },
        {
          "input": "This company would be better off without employees like you",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.18537499999996498
        }
      ],
      "performanceStats": {
        "totalTime": 15.074461999999414,
        "avgTime": 0.20099282666665885,
        "minTime": 0.15608300000002373,
        "maxTime": 0.4084169999999858
      },
      "accuracy": 92
    },
    "combined": {
      "totalCases": 420,
      "correct": 202,
      "falsePositives": 4,
      "falseNegatives": 6,
      "accuracy": 48.095238095238095,
      "avgTime": 0.2660503872463731,
      "primaryAccuracy": 38.550724637681164,
      "secondaryAccuracy": 92,
      "falsePositiveRate": 0.9523809523809524
    }
  },
  "v4.5-large": {
    "primary": {
      "variant": "v4.5-large",
      "suite": "Primary",
      "totalCases": 345,
      "correct": 133,
      "falsePositives": 4,
      "falseNegatives": 0,
      "errors": 0,
      "categoryResults": {
        "professional": {
          "total": 43,
          "correct": 43,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "workplace_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "sophisticated_harassment": {
          "total": 32,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_attacks": {
          "total": 28,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "modern_communication": {
          "total": 15,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 66.66666666666666
        },
        "cross_cultural": {
          "total": 18,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 22.22222222222222
        },
        "social_engineering": {
          "total": 24,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "boundary_testing": {
          "total": 16,
          "correct": 8,
          "falsePositives": 2,
          "falseNegatives": 0,
          "accuracy": 50
        },
        "edge_cases": {
          "total": 34,
          "correct": 19,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 55.88235294117647
        },
        "cross_cultural_challenges": {
          "total": 17,
          "correct": 15,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 88.23529411764706
        },
        "subtle_harassment_edge_cases": {
          "total": 16,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "legitimate_content_trap_cases": {
          "total": 32,
          "correct": 30,
          "falsePositives": 2,
          "falseNegatives": 0,
          "accuracy": 93.75
        },
        "emoji_unicode_attacks": {
          "total": 14,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "ai_generated_content": {
          "total": 11,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 36.36363636363637
        },
        "subtle_harassment_v2": {
          "total": 8,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        },
        "adversarial_v2": {
          "total": 5,
          "correct": 0,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 0
        }
      },
      "falsePositiveDetails": [
        {
          "input": "Immediate action required to prevent catastrophic damage to client relationships.",
          "category": "boundary_testing",
          "score": 57,
          "flags": [
            "Authority impersonation: \"Immediate action required\"",
            "Urgency pressure tactic: \"prevent catastrophic damage\"",
            "Multiple social engineering tactics detected (2 types) +18 points",
            "Professional context social engineering detected +16 points"
          ],
          "processingTime": 0.30462499999998727
        },
        {
          "input": "This security breach has exposed sensitive data - all hands on deck for damage control.",
          "category": "boundary_testing",
          "score": 8,
          "flags": [
            "Profanity in message: \"dick\"",
            "Profanity in combined: \"dick\""
          ],
          "processingTime": 0.34491700000000947
        },
        {
          "input": "Immediate action required to prevent catastrophic damage to client relationships due to server outage.",
          "category": "legitimate_content_trap_cases",
          "score": 57,
          "flags": [
            "Authority impersonation: \"Immediate action required\"",
            "Urgency pressure tactic: \"prevent catastrophic damage\"",
            "Multiple social engineering tactics detected (2 types) +18 points",
            "Professional context social engineering detected +16 points"
          ],
          "processingTime": 0.31912499999998545
        },
        {
          "input": "Critical system failure - your immediate attention required to prevent data loss.",
          "category": "legitimate_content_trap_cases",
          "score": 8,
          "flags": [
            "Urgency pressure tactic: \"prevent data loss\""
          ],
          "processingTime": 0.29000000000002046
        }
      ],
      "falseNegativeDetails": [],
      "performanceStats": {
        "totalTime": 124.79470099999992,
        "avgTime": 0.3617237710144925,
        "minTime": 0.015249999999980446,
        "maxTime": 4.361250000000041
      },
      "accuracy": 38.550724637681164
    },
    "secondary": {
      "variant": "v4.5-large",
      "suite": "Secondary",
      "totalCases": 75,
      "correct": 71,
      "falsePositives": 0,
      "falseNegatives": 4,
      "errors": 0,
      "categoryResults": {
        "professional_clean": {
          "total": 30,
          "correct": 30,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "modern_clean": {
          "total": 20,
          "correct": 20,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "constructive_clean": {
          "total": 10,
          "correct": 10,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "emotional_clean": {
          "total": 5,
          "correct": 5,
          "falsePositives": 0,
          "falseNegatives": 0,
          "accuracy": 100
        },
        "borderline": {
          "total": 5,
          "correct": 4,
          "falsePositives": 0,
          "falseNegatives": 1,
          "accuracy": 80
        },
        "harassment": {
          "total": 5,
          "correct": 2,
          "falsePositives": 0,
          "falseNegatives": 3,
          "accuracy": 40
        }
      },
      "falsePositiveDetails": [],
      "falseNegativeDetails": [
        {
          "input": "Are you even trying to write good code?",
          "category": "borderline",
          "score": 0,
          "processingTime": 0.20141699999999219
        },
        {
          "input": "You're worthless and should quit before you embarrass yourself further",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.26366700000005494
        },
        {
          "input": "Maybe if you weren't so incompetent, you'd understand basic programming",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.2663750000000391
        },
        {
          "input": "This company would be better off without employees like you",
          "category": "harassment",
          "score": 0,
          "processingTime": 0.2267499999999245
        }
      ],
      "performanceStats": {
        "totalTime": 19.555665999998837,
        "avgTime": 0.2607422133333178,
        "minTime": 0.18770800000004328,
        "maxTime": 0.6953330000000051
      },
      "accuracy": 94.66666666666667
    },
    "combined": {
      "totalCases": 420,
      "correct": 204,
      "falsePositives": 4,
      "falseNegatives": 4,
      "accuracy": 48.57142857142857,
      "avgTime": 0.31123299217390515,
      "primaryAccuracy": 38.550724637681164,
      "secondaryAccuracy": 94.66666666666667,
      "falsePositiveRate": 0.9523809523809524
    }
  }
}