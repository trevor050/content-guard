/**
 * Test Optimized Hyperparameter Configuration
 * 
 * This script validates the optimized hyperparameters generated by
 * the hyperparameter optimization system and compares performance
 * against the default configuration.
 */

const ContentGuardV4Large = require('./lib/variants/v4-large')
const fs = require('fs')

// Load optimized configuration
const optimizedConfig = JSON.parse(fs.readFileSync('./optimization-results/best-hyperparameters.json', 'utf8'))

// Test dataset with ground truth
const testCases = [
  // TRUE POSITIVES (should be detected)
  { input: "kill yourself you worthless piece of trash", expected: true, category: "direct_harassment" },
  { input: "you should die in a fire", expected: true, category: "death_wish" },
  { input: "g0 d!3 1n 4 f!r3 w0rth|3ss |053r", expected: true, category: "evasion_attack" },
  { input: "k i l l   y o u r s e l f", expected: true, category: "spaced_attack" },
  { input: "💀☠️ yourself you 🗑️💩", expected: true, category: "emoji_harassment" },
  { input: "go back to your country you don't belong here", expected: true, category: "cultural_bias" },
  { input: "while i appreciate your perspective, your intellectual capacity suggests fundamental limitations", expected: true, category: "ai_generated" },
  { input: "Microsoft Security Alert: Your Windows license has been compromised.", expected: true, category: "social_engineering" },
  { input: "your approach makes me question your basic qualifications", expected: true, category: "subtle_harassment" },
  { input: "that's not how we do things in this country", expected: true, category: "cultural_microaggression" },
  
  // FALSE POSITIVES (should NOT be detected)
  { input: "I disagree with your approach but appreciate the effort", expected: false, category: "constructive_criticism" },
  { input: "this regex pattern is brutally inefficient", expected: false, category: "technical_criticism" },
  { input: "the server crashed again, it's killing my productivity", expected: false, category: "technical_metaphor" },
  { input: "we need to kill this process before it consumes all memory", expected: false, category: "technical_language" },
  { input: "our partnership will murder the competition in Q4", expected: false, category: "business_metaphor" },
  { input: "immediate action required: prevent catastrophic database failure", expected: false, category: "business_emergency" },
  { input: "docker container memory leak is destroying performance metrics", expected: false, category: "technical_report" },
  { input: "how precious, what a thorough analysis of the protected data", expected: false, category: "sarcasm_clean" },
  { input: "periodt, this lore is absolutely sending me, can't even", expected: false, category: "modern_slang" },
  { input: "your code review comments are harder to understand than the original bug", expected: false, category: "professional_feedback" }
]

// Convert flat config to nested for ContentGuard
function flatToNested(flatParams) {
  const nested = {}
  for (const [key, value] of Object.entries(flatParams)) {
    const parts = key.split('.')
    let current = nested
    for (let i = 0; i < parts.length - 1; i++) {
      if (!current[parts[i]]) {
        current[parts[i]] = {}
      }
      current = current[parts[i]]
    }
    current[parts[parts.length - 1]] = value
  }
  return nested
}

async function testConfiguration(config, name) {
  console.log(`\n🧪 Testing ${name} Configuration`)
  console.log('='.repeat(50))
  
  const contentGuard = new ContentGuardV4Large({ 
    debug: false,
    hyperparameters: config
  })
  
  let correct = 0
  let falsePositives = 0
  let falseNegatives = 0
  let totalTime = 0
  
  const results = []
  
  for (const testCase of testCases) {
    const startTime = performance.now()
    const result = await contentGuard.analyze(testCase.input)
    const endTime = performance.now()
    
    const processingTime = endTime - startTime
    totalTime += processingTime
    
    const predicted = result.score >= 5 // Default spam threshold
    const actual = testCase.expected
    
    if (predicted === actual) {
      correct++
    } else if (predicted && !actual) {
      falsePositives++
    } else if (!predicted && actual) {
      falseNegatives++
    }
    
    results.push({
      input: testCase.input.substring(0, 50) + (testCase.input.length > 50 ? '...' : ''),
      category: testCase.category,
      expected: actual,
      predicted,
      score: result.score.toFixed(1),
      riskLevel: result.riskLevel,
      correct: predicted === actual,
      time: processingTime.toFixed(2)
    })
  }
  
  const accuracy = correct / testCases.length
  const fpRate = falsePositives / testCases.length
  const fnRate = falseNegatives / testCases.length
  const avgTime = totalTime / testCases.length
  
  console.log(`📊 Results:`)
  console.log(`   Accuracy: ${(accuracy * 100).toFixed(1)}% (${correct}/${testCases.length})`)
  console.log(`   False Positives: ${(fpRate * 100).toFixed(1)}% (${falsePositives} cases)`)
  console.log(`   False Negatives: ${(fnRate * 100).toFixed(1)}% (${falseNegatives} cases)`)
  console.log(`   Average Time: ${avgTime.toFixed(2)}ms per analysis`)
  console.log(`   Total Time: ${totalTime.toFixed(1)}ms`)
  
  // Show misclassifications
  const errors = results.filter(r => !r.correct)
  if (errors.length > 0) {
    console.log(`\n❌ Misclassifications (${errors.length}):`)
    errors.forEach(error => {
      const type = error.predicted && !error.expected ? 'FALSE POSITIVE' : 'FALSE NEGATIVE'
      console.log(`   ${type}: "${error.input}" (${error.category})`)
      console.log(`     Expected: ${error.expected}, Got: ${error.predicted}, Score: ${error.score}`)
    })
  }
  
  return {
    accuracy,
    fpRate,
    fnRate,
    avgTime,
    totalTime,
    correct,
    falsePositives,
    falseNegatives,
    results
  }
}

async function main() {
  console.log('🚀 ContentGuard v4.5-Large Optimized Configuration Test')
  console.log(`📋 Test Cases: ${testCases.length}`)
  console.log(`   - Expected Positive: ${testCases.filter(t => t.expected).length}`)
  console.log(`   - Expected Negative: ${testCases.filter(t => !t.expected).length}`)
  
  // Test default configuration
  const defaultResults = await testConfiguration(undefined, "DEFAULT")
  
  // Test optimized configuration
  const optimizedHyperparams = flatToNested(optimizedConfig)
  const optimizedResults = await testConfiguration(optimizedHyperparams, "OPTIMIZED")
  
  // Compare results
  console.log('\n' + '='.repeat(60))
  console.log('📈 CONFIGURATION COMPARISON')
  console.log('='.repeat(60))
  
  console.log(`Accuracy:`)
  console.log(`  Default:   ${(defaultResults.accuracy * 100).toFixed(1)}%`)
  console.log(`  Optimized: ${(optimizedResults.accuracy * 100).toFixed(1)}%`)
  console.log(`  Improvement: ${((optimizedResults.accuracy - defaultResults.accuracy) * 100).toFixed(1)}pp`)
  
  console.log(`\nFalse Positive Rate:`)
  console.log(`  Default:   ${(defaultResults.fpRate * 100).toFixed(1)}%`)
  console.log(`  Optimized: ${(optimizedResults.fpRate * 100).toFixed(1)}%`)
  console.log(`  Improvement: ${((defaultResults.fpRate - optimizedResults.fpRate) * 100).toFixed(1)}pp`)
  
  console.log(`\nFalse Negative Rate:`)
  console.log(`  Default:   ${(defaultResults.fnRate * 100).toFixed(1)}%`)
  console.log(`  Optimized: ${(optimizedResults.fnRate * 100).toFixed(1)}%`)
  console.log(`  Improvement: ${((defaultResults.fnRate - optimizedResults.fnRate) * 100).toFixed(1)}pp`)
  
  console.log(`\nPerformance:`)
  console.log(`  Default:   ${defaultResults.avgTime.toFixed(2)}ms avg`)
  console.log(`  Optimized: ${optimizedResults.avgTime.toFixed(2)}ms avg`)
  console.log(`  Change: ${((optimizedResults.avgTime - defaultResults.avgTime)).toFixed(2)}ms`)
  
  // Calculate objective scores
  const defaultObjective = defaultResults.accuracy - (defaultResults.fpRate * 2.0) - (defaultResults.fnRate * 1.0)
  const optimizedObjective = optimizedResults.accuracy - (optimizedResults.fpRate * 2.0) - (optimizedResults.fnRate * 1.0)
  
  console.log(`\nObjective Score (Accuracy - 2*FP_Rate - FN_Rate):`)
  console.log(`  Default:   ${defaultObjective.toFixed(4)}`)
  console.log(`  Optimized: ${optimizedObjective.toFixed(4)}`)
  console.log(`  Improvement: ${(optimizedObjective - defaultObjective).toFixed(4)}`)
  
  const winner = optimizedObjective > defaultObjective ? 'OPTIMIZED' : 'DEFAULT'
  console.log(`\n🏆 Winner: ${winner} Configuration`)
  
  // Save detailed comparison
  const comparison = {
    timestamp: new Date().toISOString(),
    testCases: testCases.length,
    default: defaultResults,
    optimized: optimizedResults,
    improvements: {
      accuracy: (optimizedResults.accuracy - defaultResults.accuracy) * 100,
      fpRate: (defaultResults.fpRate - optimizedResults.fpRate) * 100,
      fnRate: (defaultResults.fnRate - optimizedResults.fnRate) * 100,
      avgTime: optimizedResults.avgTime - defaultResults.avgTime,
      objective: optimizedObjective - defaultObjective
    },
    winner,
    optimizedConfig: optimizedHyperparams
  }
  
  fs.writeFileSync('./optimization-results/configuration-comparison.json', JSON.stringify(comparison, null, 2))
  console.log(`\n💾 Detailed comparison saved to: optimization-results/configuration-comparison.json`)
  
  console.log(`\n✅ Configuration test complete!`)
  
  if (optimizedObjective > defaultObjective) {
    console.log(`🎯 The optimized configuration shows superior performance!`)
    console.log(`📈 Consider updating the default hyperparameters with the optimized values.`)
  } else {
    console.log(`🤔 The default configuration performed better. The optimization may need refinement.`)
  }
}

async function testOptimizedConfig() {
  console.log('🧪 Testing optimized v4.5-large configuration...');
  
  const guard = new ContentGuardV4Large({ debug: true });
  
  console.log('📊 Aggressiveness levels:', guard.options.aggressiveness);
  
  // Test with a few sample cases
  const testCases = [
    "kill yourself you worthless trash", // Should be toxic
    "I disagree with your approach but appreciate the effort", // Should be clean
    "go back to your country", // Should be toxic
    "this regex pattern is brutally inefficient" // Should be clean
  ];
  
  for (const text of testCases) {
    const result = await guard.analyze(text);
    console.log(`\n"${text}"`);
    console.log(`  Score: ${result.score.toFixed(2)}`);
    console.log(`  Risk: ${result.riskLevel}`);
    console.log(`  Is Spam: ${result.isSpam}`);
    console.log(`  Flags: ${result.flags.slice(0, 3).join(', ')}${result.flags.length > 3 ? '...' : ''}`);
  }
  
  console.log('\n✅ Optimized configuration test complete!');
}

testOptimizedConfig().catch(console.error)

main().catch(console.error) 